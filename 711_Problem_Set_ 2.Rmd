---
title: "Predictive Analytics - Problem Set -2"
author: "Daniel D. Mondal"
date: "2026-02-05"
output: word_document
---

### 1. Problem to demonstrate that the population regression line is fixed, but least square regression line varies.

Suppose the population regression line is given by $Y = 2 + 3x$, while the data
comes from the model $y = 2 + 3x +  ϵ$.

**Step 1:** For x in the range [5,10] graph the population regression line.

**Step 2:** Generate $x_i(i = 1, 2, .., n)$ from Uniform (5, 10) and $ϵ_i (i = 1, 2, .., n)$ from $N(0, 4^2)$.
Hence, compute $y_1, y_2, .., y_n$.

**Step 3:** On the basis of the data $(x_i, y_i)(i = 1, 2, .., n)$ 
generated in **Step 2.**
report the least squares regression line.

**Step 4:** Repeat steps 2-3 five times. Graph the 5 least squares regression lines
over the population regression line obtained in Step 1. Interpret the findings.
Take n = 50. Set the seed as seed=123.

### Solution 1:

The population regression function is defined as:$$ Y = 2 + 3x $$
The sample regrssion function including the random error can be defines as:$$ y_i = 2 + 3x_i +  ϵ_i $$

Where:

* $x_i \sim \text{Uniform}(5, 10)$
* $ϵ_i \sim N(0, 4^2)$


```{r}
rm(list=ls())
set.seed(123)

x = seq(5,10,length.out= 200)
y = 2+3*x
plot(x,y,col = "black", lwd = 2,
     main = "Comparision of PRF and SRF",type="l")
beta_hat = matrix(0,1,2)
for(i in 1:5)
{
  x = runif(50,5,10)
  e = rnorm(50,0,4)
  y_sam = 2+ 3*x+ e
 
  model = lm(y_sam~x)
  summary(model)
  beta_hat = rbind(beta_hat,model$coefficients)
  lines(x,predict(model), col= i+1,lwd=1)
}
legend("topleft",legend= c("PRF",paste("SRF",1:5)),fill = c("black",2:6),border = "black")

print("The estimates in 5 cases are given as:")
beta_hat[-1,]
```

#### Interpretation
Although the population regression line is fixed, the least squares regression line varies from sample to sample due to random error (sampling fluctuations).


### 2. Problem to demonstrate that $β_0$ and $β$ min imises RSS
**Step 1:** Generate $x_i$ from $Uniform(5, 10)$ and mean centre the values. Generate $ϵ_i$ from $N(0,1)$. 
Calculate $y_i = 2 + 3x_i +  ϵ_i$,
$i = 1,2,.., n$. Take n=50 and seed=123.

**Step 2:** Now imagine that you only have the data on 
$(x_i,y_i),i = 1,2,..,n$ ,without knowing the mechanism that was used to generate the data in step 1.

Assuming a linear regression of the type $y_i = β_0 +β x_i +  ϵ_i$, and based on these data $(x_i,y_i)$ , $i = 1,2,..,n$. Obtain the least squares estimates of $β_0$ and $β$.

**Step 3:** Take a large number of grid values of $(β_0,β)$ that also include the least squares estimates obtained from step 2. Compute the RSS for each parametric choice of $(β_0,β)$. 

where:

$RSS = (y_1 −β_0 −β x_1)^2 +(y_2 −β_0 −β x_2)^2 +....+(yn −β_0 −β x_n)^2$.

Find out for which combination of $(β_0,β)$, RSS is minimum.

### Solution 2:
```{r}
rm(list=ls())
set.seed(123)

x = runif(50,5,10)
x_mc = x - mean(x)

e = rnorm(50,0,1)
y_sam = 2 + 3*x_mc + e

b_0 = seq(1,3, 0.01)
b_1 = seq(2,4, 0.01)

b = expand.grid(b_0,b_1)
head(b)

rss = numeric(length(b[,1])) # Pre-allocate vector for speed

for(i in 1:length(b[,1])) {
  # RSS formula: sum of squared residuals
  rss[i] = sum((y_sam - (b[i,1] + b[i,2] * x_mc))^2)
}
output = data.frame(b,rss)
colnames(output) = c("B_0","B_1","RSS")

print("Minimum RRS and the values of the estimates")
output[which.min(output$RSS), ]


coefs = lm(y_sam ~ x_mc)$coefficients
rss_min = sum((y_sam - (coefs[1] + coefs[2]*x_mc))^2)
rss_min
```

We see that the RSS is minimum at the least squares estimates of $β_0$ and $β$.


### 3. Problem to demonstrate that least square estimators are unbiased

**Step 1:** Generate $x_i(i = 1,2,..,n)$ from $Uniform(0,1)$, $ϵ_i (i = 1,2,..,n)$ from $N(0,1)$ and hence generate y using $yi = β_0 +βx_i +ϵ_i$.

$(Take:  β_0 = 2,β = 3)$.

**Step 2:** On the basis of the data $(x_i,y_i)(i = 1,2,..,n)$ generated in Step 1, obtain the least square estimates of $\hat{β_0}$ and $\hat{β}$.
Repeat Steps 1-2, R = 1000 times. 
In each simulation obtain $\hat{β_0}$ and $\hat{β}$. 

Finally, the least-square estimates will be given by the average of these estimated values.
Compare these with the true $β_0 and β$ and comment.
Take n = 50 and seed = 123.

### Solution 3
```{r}
rm(list=ls())
set.seed(123)
beta_hat = matrix(0,1,2)
for(i in 1:1000)
{
  x = runif(50,5,10)
  e = rnorm(50,0,4)
  y_sam = 2+ 3*x+ e
 
  model = lm(y_sam~x)
  summary(model)
  beta_hat = rbind(beta_hat,model$coefficients)
}
apply(beta_hat[-1,],2,mean)
```

#### Comment
The average of the estimated coefficients, 
i.e. empirical $E(\hat{β_0}) = 2.055461$ and $E(\hat{β}) = 3$ is close to the true parameter values $β_0 = 2$ and $β = 2.993070$, demonstrating that in the long run, $\hat{β_0}$ and   $\hat{β}$ are unbiased for $β_0 and β$.


### 4. Comparing several simple linear regressions

Attach “Boston” data from MASS library in R. Select median value of owner occupied homes, as the response and per capita crime rate, nitrogen oxides concentration, proportion of blacks and percentage of lower status of the population as predictors:

(a) Selecting the predictors one by one, run four separate linear regressions to the data. Present the output in a single table.

(b) Which model gives the best fit?

(c) Compare the coefficients of the predictors from each model and comment on the usefulness of the predictors.

### Solution 4

Response Variable ($Y$):


* medv: Median value of owner-occupied homes.

Predictor Variables ($X$):

* crim: Per capita crime rate.
* nox: Nitrogen oxides concentration.
* black: Proportion of black people.
* lstat: Percentage of lower status of the population.


```{r}
library(MASS)
data = Boston
attach(data)
library(stargazer)

sub_data = data.frame(medv,crim,nox,black,lstat)
head(sub_data)

model1 = lm(medv~crim)
model2 = lm(medv~nox)
model3 = lm(medv~black)
model4 = lm(medv~lstat)

stargazer(model1,model2,model3,model4,type="text")
```

Using the stargazer function, we obtain a table containing the simple linear regression coefficient and other summary measures.

#### Comments

* Here model4, where medv is explained by lstat, has the highest $R^2$ value, 0.544 .
* The coefficients indicate the direction and strength of the relationship between the response and each predictor. Among these, {lstat} shows the strongest association with {medv}.
* With the stargazer package we can see the number of stars on the estimates and comment that each of simple linear equations are significant in the t-tests at 0.01 level of significance.
